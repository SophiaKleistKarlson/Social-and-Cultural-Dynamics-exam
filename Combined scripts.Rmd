---
title: "data-cleaning, preparing models and running models"
output: html_document
author: Sophia Marthine Kleist Karlson
date: May 17, 2020
---


Data cleaning


```{r}
library(pacman)
p_load(tidyverse)

setwd("C:/Users/Sophia/Documents/Karsten projekt/exam/data")

data <- read_csv("data.csv") #load the data



new_df <- as.data.frame(t(data)) #swap the columns and rows

any(is.na(new_df))#check if there is any NA's in the df

row.names(new_df) <- NULL #remove the rownames (reflecting subject id, but in a stupid way)

new_df$Subject_ID <- rep(1:39, each=240) #make a new column with subject id - 39 subjects, each with 240 trials

new_df <- as.tibble(new_df) #make the df into a tibble


colnames(new_df) <- c("Age", #rename columns
                      "Gender", 
                      "Session", 
                      "Trial", 
                      "Advisor", 
                      "stm_Coh",
                      "stm_Dir",
                      "Decicion_1",
                      "Confidence_1",
                      "RT_1",
                      "Accuracy_1",
                      "Score_1",
                      "Score_2",
                      "Decision_2",
                      "Confidence_2",
                      "RT_2",
                      "Accuracy_2",
                      "avd_Decision",
                      "adv_Confidence",
                      "adv_Accuracy",
                      "Subject_ID")

write.csv(new_df,"cleaned data.csv") #save the data

```



### Wipe environment###



Preparing variables (resolution, benefit, resolution, gender)


```{r}
setwd("~/Karsten projekt/exam/data")

library(pacman)
p_load(tidyversE)


data <- read_csv("cleaned data.csv") # We use the cleaned data
data$X1 <- NULL # Removing the first unnecessary column
```


Resolution
Resolution = mean_abs_confidence_correct - mean_abs_confidence_incorrect
```{r}
# Make confidence 1 into absolute confidence
data$abs_conf_1 <- abs(data$Confidence_1)

# Lists of absolute confidence for incorrect and correct answers with subject ID
incorrect_conf <- data %>% filter(Accuracy_1=="0") %>% group_by(Subject_ID) %>% select(abs_conf_1)
correct_conf <- data %>% filter(Accuracy_1=="1") %>% group_by(Subject_ID) %>% select(abs_conf_1)

# As dataframes - not sure if this is strictly necessary
incorrect_conf <- as.data.frame(incorrect_conf)
correct_conf <- as.data.frame(correct_conf)

# Making an empty list to fill in the resolution scores
Resolution <- c()

# Loop for finding resolution for each participant
for (i in 1:39) {
  
  inc_ID <- incorrect_conf %>% filter(Subject_ID == i) # Take the absolute confidence for incorrect answers for participant i
  cor_ID <- correct_conf %>% filter(Subject_ID == i) # Take the absolute confidence for correct answers for participant i
  
  Res <- mean(cor_ID$abs_conf_1) - mean(inc_ID$abs_conf_1) # resolution = mean of confidence for correct minus ditto of incorrect
  Resolution[i] <- paste(Res, collapse=NULL) # Pate resolution score into the list
}

# Print resolution scores for all 39 participants
print(Resolution) 

# Make Resolutions into numeric instead of character
class(Resolution)
Resolution <- as.numeric(Resolution)

# Take a look at range and mean of Resolutions
range(Resolution)
mean(Resolution)

# Add subject ID to the Resolution dataframe
Subject_ID <- c(1:39)
Resolution <- data.frame(Subject_ID, Resolution)

# Save a seperate data-file of the resolution scores
write.csv(Resolution, "Resolution.csv")
```


Benefit
Benefit = score_2 - score_1
```{r}
# Add a column called Benefit to data, calculated by subtracting score_1 from score_2
data$Benefit <- data$Score_2 - data$Score_1

# Take a look at range and mean of benefit
range(data$Benefit)
mean(data$Benefit)
```



Calibration
Calibration = sum of differences between confidence level (ranging 0.6 to 1.0) and mean accuracy (ranging 0.0 to 1.0) for each step on the confidence scale.
```{r}
# Selecting necessary variables
cal_df <- data %>% group_by(Subject_ID) %>% select(abs_conf_1, Accuracy_1)

# Make empty list for calibration scores
Calibration <- c()


# Loop for finding calibration for each participant
for (i in 1:39){
  cal_0.6 <- cal_df %>% filter(Subject_ID == i & abs_conf_1 == 0.6) # make a df when absolute confidence for subject i is 0.6 
  cal_0.7 <- cal_df %>% filter(Subject_ID == i & abs_conf_1 == 0.7) # make a df when absolute confidence for subject i is 0.7
  cal_0.8 <- cal_df %>% filter(Subject_ID == i & abs_conf_1 == 0.8) # make a df when absolute confidence for subject i is 0.8 
  cal_0.9 <- cal_df %>% filter(Subject_ID == i & abs_conf_1 == 0.9) # make a df when absolute confidence for subject i is 0.9 
  cal_1.0 <- cal_df %>% filter(Subject_ID == i & abs_conf_1 == 1.0) # make a df when absolute confidence for subject i is 1.0 
  
  # find mean accuracy_1 for each level of the confidence scale
  mean_0.6 <- mean(cal_0.6$Accuracy_1) 
  mean_0.7 <- mean(cal_0.7$Accuracy_1)
  mean_0.8 <- mean(cal_0.8$Accuracy_1)
  mean_0.9 <- mean(cal_0.9$Accuracy_1)
  mean_1.0 <- mean(cal_1.0$Accuracy_1)
  
  mean_acc <- c(mean_0.6, mean_0.7, mean_0.8, mean_0.9, mean_1.0) #list of mean accuracies for each level of the confidence scale
  conf <- c(0.6, 0.7, 0.8, 0.9, 1.0) # list of confidence levels
  Calibrations <- sum(conf - mean_acc, na.rm = T) # calculate calibration: sum of differences between confidence level and mean accuracy for that confidence level
  Calibration[i] <- paste(Calibrations, collapse=NULL) # paste into the calibration list
}

# Print calibration scores for all 39 participants
print(Calibration)

#Making a df with one measure of calibration per participant
Subject_ID <- c(1:39)
Calibration <- data.frame(Subject_ID, Calibration)

#making calibration into numeric
class(Calibration$Calibration)
Calibration$Calibration <- as.character(Calibration$Calibration) #this is needed
Calibration$Calibration <- as.numeric(Calibration$Calibration) #making it into numeric
class(Calibration$Calibration)


# Take a look at range and mean of calibration
mean(Calibration$Calibration)
range(Calibration$Calibration)

# Save a seperate data-file of calibration scores
write.csv(Calibration, "Calibration.csv")
```


Gender
```{r}
# To make a list of the genders of the 39 participants

Genders <- c(0)
for (i in 1:39){
  ID <- data %>% filter(Subject_ID == i) 
  Gender <- mean(ID$Gender)
  Genders[i] <- paste(Gender, collapse=NULL)
}
print(Genders)

# Make a list of participants
Subject_ID <- c(1:39)

# Make a dataframe with participants and their gender
Gender <- data.frame(Subject_ID, Genders)

# Renaming "Genders" to "Gender"
Gender$Gender <- Gender$Genders
Gender$Genders <- NULL
```


Beautiful datasets
```{r}
# Make a new dataframe only with meta-cogtinive measures, by merging the calibration dataframe and resolution dataframe
df_meta <- merge(Calibration, Resolution)

# New dataset with metacognitive measures as well
data_1 <- merge(data, df_meta)

# Add gender to df_meta
df_meta <- merge(Gender, df_meta)

# Save df_meta and data_1
write.csv(df_meta, "Meta-measures.csv")
write.csv(data_1, "data_1.csv")
```



### Wipe environment###



Model 0 - calculating susceptibility 


```{r}
setwd("~/Karsten projekt/exam/data")

library(pacman)
p_load(tidyverse, brms, ggplot2)

data <- read_csv("data_1.csv") #we use data_1
data$X1 <- NULL#removing the first unnecessary column
```


Preparing the data and checking stuff
```{r}
# Checking classes
class(data$Advisor)
class(data$Confidence_1)
class(data$Confidence_2)
class(data$adv_Confidence)
class(data$Accuracy_1)

# Advisor and Accuracy_1 should be factors
data$Accuracy_1 <- as.factor(data$Accuracy_1)
data$Advisor <- as.factor(data$Advisor)


# Chose the variables needed for the model
df_sus <- data %>% select(Subject_ID, Confidence_1, Confidence_2, Accuracy_1, Advisor, adv_Confidence)


# First, rescale confidence
df_sus$Confidence_1 <- as.factor(df_sus$Confidence_1) # Make confidence_1 into factor
levels(df_sus$Confidence_1) <- c("0.1", "0.2", "0.3", "0.4", "0.5", "0.6", "0.7", "0.8", "0.9", "1.0") # Rename levels
df_sus$Confidence_1 <- as.character(df_sus$Confidence_1) # this is necessary for some reason
df_sus$Confidence_1 <- as.numeric(df_sus$Confidence_1) # back to numeric
range(df_sus$Confidence_1) # Checking that the range is 0.1 to 1.0
class(df_sus$Confidence_1) #Checking that it's numeric

# Same deal for Confidence_2 and adv_Confidence
df_sus$Confidence_2 <- as.factor(df_sus$Confidence_2)
levels(df_sus$Confidence_2) <- c("0.1", "0.2", "0.3", "0.4", "0.5", "0.6", "0.7", "0.8", "0.9", "1.0")
df_sus$Confidence_2 <- as.character(df_sus$Confidence_2)
df_sus$Confidence_2 <- as.numeric(df_sus$Confidence_2)
range(df_sus$Confidence_2)
class(df_sus$Confidence_2)

df_sus$adv_Confidence <- as.factor(df_sus$adv_Confidence)
levels(df_sus$adv_Confidence) <- c("0.1", "0.2", "0.3", "0.4", "0.5", "0.6", "0.7", "0.8", "0.9", "1.0")
df_sus$adv_Confidence <- as.character(df_sus$adv_Confidence)
df_sus$adv_Confidence <- as.numeric(df_sus$adv_Confidence)
range(df_sus$adv_Confidence)
class(df_sus$adv_Confidence)
```


Mod 0 
Confidence_2 ~ 1 + Confidence_1 + Accuracy_1 + Advisor + adv_Confidence + Confidence_1:Accuracy_1 + (Confidence_1 + Advisor):adv_Confidence + (1 + Confidence_1 + Accuracy_1 + Advisor + adv_Confidence + Confidence_1:Accuracy_1 + (Confidence_1 + Advisor):adv_Confidence | Subject_ID)
```{r}
# Set seed so that the analysis can be re-run and give the same results
set.seed(555)

# Define the model
sus_mod <- bf(Confidence_2 ~ 1 + Confidence_1 + Accuracy_1 + Advisor + adv_Confidence + Confidence_1:Accuracy_1 + (Confidence_1 + Advisor):adv_Confidence + (1 + Confidence_1 + Accuracy_1 + Advisor + adv_Confidence + Confidence_1:Accuracy_1 + (Confidence_1 + Advisor):adv_Confidence | Subject_ID))


# Figure out what priors we'll need
get_prior(sus_mod, family = gaussian, df_sus)

# Checking range, mean and standard deviation of confidence_2, to determine which family to choose and to use for beta- and intercept-priors
range(df_sus$Confidence_2)
mean(df_sus$Confidence_2)
sd(df_sus$Confidence_2)

# For choosing the sd prior
df_part <- df_sus %>% group_by(Subject_ID) %>% summarize(conf_2_mean = mean(Confidence_2)) #find mean confidence_2 for each participant
sd(df_part$conf_2_mean) #get the standard deviation of the mean confidence_2 for each participant


# Define priors
prior_sus_mod <- c(
  prior(normal(0.55, .23),     class = b), #mean and sd of confidence 2
  prior(lkj(1),                class = cor),
  prior(normal(.56, .31),      class = Intercept), #mean and sd of confidence 2
  prior(normal(0, .02),        class = sd), #mean: 0 (I expect that they might not vary at all). sd for the mean conf_2 for each participant = 0.045. sigma should go from 0 (the mean of the prior) to around that -> sigma: 0.02.
  prior(normal(.23, .15),      class = sigma) #mean: sd of confidence 2, sigma: half of the sd of confidence 2
)


# Fitting the model samplig only from priors, in order to check the quality of the priors
skep_sus_mod0 <- brm(
  formula = sus_mod, 
  prior = prior_sus_mod,
  data = df_sus,
  chains = 2,
  cores = 2,
  sample_prior = "only"
)


# Prior predictive check
pp_check(skep_sus_mod0, nsamples = 1000) # 


# The actual model:
skep_sus_mod1 <- brm(
  formula = sus_mod, 
  prior = prior_sus_mod,
  data = df_sus,
  chains = 2,
  cores = 2,
  sample_prior = T,
  iter = 4000,
  control = list(adapt_delta=0.99, max_treedepth=20)
)

# Posterior predictive check
pp_check(skep_sus_mod1, nsamples = 1000)


# Model summary
summary(skep_sus_mod1) # Warnings? Suspicious Rhat activity? Bad priors?

# Plot the model to get trace plots
plot(skep_sus_mod1)


# Rank trace plots
mcmc_rank_overlay(skep_sus_mod1, 
                  pars = c("b_Confidence_1", "b_Accuracy_11", "b_Advisor2", "b_adv_Confidence")) + 
  theme_classic() #, "b_Advisor2:adv_Confidence"

mcmc_rank_overlay(skep_sus_mod1, 
                  pars = c("b_Confidence_1:Accuracy_11", "b_Confidence_1:adv_Confidence", "b_Advisor2:adv_Confidence", "sd_Subject_ID__Confidence_1")) + 
  theme_classic()

mcmc_rank_overlay(skep_sus_mod1, 
                  pars = c("sd_Subject_ID__Accuracy_11", "sd_Subject_ID__Advisor2", "sd_Subject_ID__adv_Confidence", "sd_Subject_ID__Confidence_1:Accuracy_11")) + 
  theme_classic()

mcmc_rank_overlay(skep_sus_mod1, 
                  pars = c("sd_Subject_ID__Confidence_1:adv_Confidence", "sd_Subject_ID__Advisor2:adv_Confidence")) + 
  theme_classic()



# Check model learning for betas and sd's
plot(hypothesis(skep_sus_mod1,"Intercept > 0"))
plot(hypothesis(skep_sus_mod1,"Confidence_1 > 0"))
plot(hypothesis(skep_sus_mod1,"Accuracy_11 > 0"))
plot(hypothesis(skep_sus_mod1,"Advisor2 > 0"))
plot(hypothesis(skep_sus_mod1,"adv_Confidence > 0"))
plot(hypothesis(skep_sus_mod1,"Confidence_1:Accuracy_11 > 0"))
plot(hypothesis(skep_sus_mod1,"Confidence_1:adv_Confidence > 0"))
plot(hypothesis(skep_sus_mod1,"Advisor2:adv_Confidence > 0"))

plot(hypothesis(skep_sus_mod1,"Intercept > 0", class="sd", group="Subject_ID"))
plot(hypothesis(skep_sus_mod1,"Confidence_1 > 0", class="sd", group="Subject_ID"))
plot(hypothesis(skep_sus_mod1,"Accuracy_11 > 0", class="sd", group="Subject_ID"))
plot(hypothesis(skep_sus_mod1,"Advisor2 > 0", class="sd", group="Subject_ID"))
plot(hypothesis(skep_sus_mod1,"adv_Confidence > 0", class="sd", group="Subject_ID"))
plot(hypothesis(skep_sus_mod1,"Confidence_1:Accuracy_11 > 0", class="sd", group="Subject_ID"))
plot(hypothesis(skep_sus_mod1,"Confidence_1:adv_Confidence > 0", class="sd", group="Subject_ID"))
plot(hypothesis(skep_sus_mod1,"Advisor2:adv_Confidence > 0", class="sd", group="Subject_ID"))

# Plot effects
conditional_effects(skep_sus_mod1)
#plot(  , class="sd", group="Subject_ID", spaghetti = T, nsamples = 50), points=T)
```


Adding susceptibility to the dataframe
```{r}
# Take the estimates of the varying effect of Advisor2:adv_Confidence
sus_crit <- ranef(
  skep_sus_mod1,
  summary = TRUE,
  robust = FALSE,
  probs = c(0.025, 0.975),
  pars = "Advisor2:adv_Confidence",
  groups = NULL
) 

# Take a look
head(sus_crit)

sus_crit <- as.data.frame(sus_crit$Subject_ID, sus_crit$Estimate) # Make a dataframe with subject ID and estimates from the varying effects of Advisor2:adv_Confidence

sus_crit$sus_crit <- sus_crit$'Estimate.Advisor2:adv_Confidence' # Rename this column to sus_crit
sus_crit$'Estimate.Advisor2:adv_Confidence' <- NULL # Delete the old column

# Look at sus_crit
mean(sus_crit$sus_crit)
sd(sus_crit$sus_crit)
summary(sus_crit$sus_crit)

# Make a list of participants
Subject_ID <- c(1:39)

# Add participants to sus_crit
sus_crit$Subject_ID <- Subject_ID

# Save sus_crit as its own data file
write.csv(sus_crit, "crit_sus.csv")


# Make a new dataframe only with the mean estimate of critical susceptibility from sus_crit
sus_crit_1 <- sus_crit$sus_crit
sus_crit_1 <- as.data.frame(sus_crit_1)

# Add Subject_ID to the dataframe
sus_crit_1$Subject_ID <- Subject_ID


# Add susceptibility to the main dataframe ("data")
data$crit_Susceptibility <- sus_crit_1$sus_crit
write.csv(data, "data_2.csv") # Save as data_2


# Add susceptibility to the meta-measures dataframe
df_meta <- read.csv("Meta-measures.csv")
df_meta$X1 <- NULL

df_meta$crit_Susceptibility <- sus_crit_1$sus_crit_1
write.csv(df_meta, "Meta-measures.csv")
```


plot susceptibility
```{r}
plot_sus <- ggplot(sus_crit_1, aes(Subject_ID, sus_crit_1)) +
  geom_point() +
  labs(y= "Susceptibility", x = "Participant")
plot_sus
```



### Wipe environment###



Models 1-6


```{r}
#use data_2 (has the same columns as data, plus columns for res, cal, sus, gen and ben)
setwd("~/Karsten projekt/exam/data")

library(pacman)
p_load(tidyverse, brms, bayesplot)

data <- read_csv("data_2.csv")
data$X1 <- NULL #removing the first unnecessary column

df_meta <- read_csv("Meta-measures.csv") #we use Meta-measures because it only has the columns res, cal, sus and gen (meaning only one data point per participant)
df_meta$X1 <- NULL #removing the first unnecessary column


#checking the class of gender in the two dataframes - as they are numeric, I change them to factor
class(df_meta$Gender)
df_meta$Gender <- as.factor(df_meta$Gender)

class(data$Gender)
data$Gender <- as.factor(data$Gender) 
```


Mod 1
Benefit ~ 0 + Gender
Hyp: Weak positive effect of women (we have two a backdoors open, in the form of susceptibility and calibration)
```{r}
# Chose the variables needed.
df_1 <- data %>% select(Gender, Benefit)

# Set seed so that the analysis can be re-run and give the same results
set.seed(555)

# Define the model
mod_1 <- bf(Benefit ~ 0 + Gender)


# Figure out what priors we'll need:
get_prior(mod_1, family = gaussian, df_1)

#Checking range, mean and standard deviation of benefit, to find priors and determine which family to choose.
range(data$Benefit)
mean(data$Benefit) 
sd(data$Benefit)

# Define priors
prior_mod_1 <- c(
  prior(normal(7.21, 31.31),   class = b), #mean and sd of benefit
  prior(normal(31.31, 15.66),  class = sigma) #mean: sd of benefit. sigma: sd of benefit/2
)

# Fitting the model samplig only from priors, in order to check the quality of the priors
mod_1.0 <- brm(
  formula = mod_1, 
  prior = prior_mod_1,
  data = df_1,
  chains = 2,
  cores = 2,
  sample_prior = "only"
)

# Prior predictive check
pp_check(mod_1.0, nsamples = 1000) 


# The actual model:
mod_1.1 <- brm(
  formula = mod_1, 
  prior = prior_mod_1,
  data = df_1,
  chains = 2,
  cores = 2,
  sample_prior = T,
  iter = 4000,
  control = list(adapt_delta=0.99, max_treedepth=20)
)

# Posterior predictive check
pp_check(mod_1.1, nsamples = 1000)

# Model summary
summary(mod_1.1) # Warnings? Suspicious Rhat activity? Bad priors?

# Plot the model to get trace plots
plot(mod_1.1)


# Hypothesis testing
hypothesis(mod_1.1,"Gender1 > Gender0") # Do women have higher benefit? no

plot(hypothesis(mod_1.1,"Gender1 = Gender0")) # After trying different hypotheses, this turned out to be the best


# Rank trace plots
mcmc_rank_overlay(mod_1.1, 
                  pars = c("b_Gender0", "b_Gender1")) + 
  theme_classic()

# Plot conditional effects
conditional_effects(mod_1.1)
```


Mod 2
Susceptibility ~ 0 + Gender
Hyp: Weak positive effect of women (we have a backdoor open, in the form of calibration)
```{r}
# Chose the variables needed.
df_2 <- df_meta %>% select(crit_Susceptibility, Gender)

# Set seed so that the analysis can be re-run and give the same results
set.seed(555)

# Define the model
mod_2 <- bf(crit_Susceptibility ~ 0 + Gender)

# Figure out what priors we'll need:
get_prior(mod_2, family = gaussian, df_2)

# To get an idea of how to put the priors
range(df_2$crit_Susceptibility)
mean(df_2$crit_Susceptibility)
sd(df_2$crit_Susceptibility)

# Define priors
prior_mod_2 <- c(
  prior(normal(0, .1),     class = b), #mean and sd of sus
  prior(normal(.1, .05),   class = sigma) #mean: sd of sus. sigma: sd of sus/2
)


# Fitting the model samplig only from priors, in order to check the quality of the priors
mod_2.0 <- brm(
  formula = mod_2, 
  prior = prior_mod_2,
  data = df_2,
  chains = 2,
  cores = 2,
  sample_prior = "only"
)

# Prior predictive check
pp_check(mod_2.0, nsamples = 1000) # 


# The actual model:
mod_2.1 <- brm(
  formula = mod_2, 
  prior = prior_mod_2,
  data = df_2,
  chains = 2,
  cores = 2,
  sample_prior = T,
  iter = 4000,
  control = list(adapt_delta=0.99, max_treedepth=20)
)

# Posterior predictive check
pp_check(mod_2.1, nsamples = 1000)

# Model summary
summary(mod_2.1)

# Plot model to get trace plots
plot(mod_2.1) 


# Hypothesis testing
plot(hypothesis(mod_2.1,"Gender1 > Gender0")) # Initial hypothesis: Women have higher sus than men - rejected
plot(hypothesis(mod_2.1,"Gender1 < Gender0")) # After trying different hypotheses, this turned out to be the best


# Rank trace plots
mcmc_rank_overlay(mod_2.1, 
                  pars = c("b_Gender0", "b_Gender1")) + 
  theme_classic()

# Plot conditional effects
conditional_effects(mod_2.1)
```


Mod 3
Calibration ~ 0 + Gender
Hyp: Positive effect of men
```{r}
# Chose the variables needed.
df_3 <- df_meta %>% select(Calibration, Gender)

# Set seed so that the analysis can be re-run and give the same results
set.seed(555)

# Define the model
mod_3 <- bf(Calibration ~ 0 + Gender)


# Figure out what priors we'll need:
get_prior(mod_3, family = gaussian, df_3)# We get Beta and Sigma as usual

# To get an idea of how to put the priors
range(df_3$Calibration)
mean(df_3$Calibration) 
sd(df_3$Calibration) #31.31 - so 2 sd's is around 62.6
 
# Define priors
prior_mod_3 <- c(
  prior(normal(.22, .54), class = b), #mean and sd of calibration
  prior(normal(.54, .27), class = sigma) #mean: sd of calibration. sigma: sd of calibration/2
)


# Fitting the model samplig only from priors, in order to check the quality of the priors
mod_3.0 <- brm(
  formula = mod_3, 
  prior = prior_mod_3,
  data = df_3,
  chains = 2,
  cores = 2,
  sample_prior = "only",
  iter = 4000,
  control = list(adapt_delta=0.99, max_treedepth=20)
)

# Prior predictive check
pp_check(mod_3.0, nsamples = 1000) # 


# The actual model:
mod_3.1 <- brm(
  formula = mod_3, 
  prior = prior_mod_3,
  data = df_3,
  chains = 2,
  cores = 2,
  sample_prior = T
)

# Posterior predictive check 
pp_check(mod_3.1, nsamples = 1000)

# Model summary
summary(mod_3.1)

# Plot model to get trace plots
plot(mod_3.1)  
 

# Hypothesis testing
plot(hypothesis(mod_3.1,"Gender0 > Gender1")) #I hypothized that men have higher calibration than women - which is the case.

# Rank trace plots
mcmc_rank_overlay(mod_3.1, 
                  pars = c("b_Gender0", "b_Gender1")) + 
  theme_classic()

# Plot conditional effects
conditional_effects(mod_3.1)
```


Mod 4
Resolution ~ 0 + Gender
Hyp: No effect
```{r}
# Chose the variables needed.
df_4 <- df_meta %>% select(Resolution, Gender)

# Set seed so that the analysis can be re-run and give the same results
set.seed(555)#Necessary???

# Define the model
mod_4 <- bf(Resolution ~ 0 + Gender)


# Figure out what priors we'll need:
get_prior(mod_4, family = gaussian, df_4)

# To get an idea of how to put the priors
range(df_4$Resolution)
mean(df_4$Resolution)
sd(df_4$Resolution)

# Define priors
prior_mod_4 <- c(
  prior(normal(.07, .06), class = b), #mean and sd of resolution
  prior(normal(.06, .03), class = sigma) #mean: sd of resolution. sigma: sd of resolution/2
)


# Fitting the model samplig only from priors, in order to check the quality of the priors
mod_4.0 <- brm(
  formula = mod_4, 
  prior = prior_mod_4,
  data = df_4,
  chains = 2,
  cores = 2,
  sample_prior = "only"
)

# Prior predictive check
pp_check(mod_4.0, nsamples = 1000) 


# The actual model:
mod_4.1 <- brm(
  formula = mod_4, 
  prior = prior_mod_4,
  data = df_4,
  chains = 2,
  cores = 2,
  sample_prior = T,
  iter = 4000,
  control = list(adapt_delta=0.99, max_treedepth=20)
)

# Posterior predictive check
pp_check(mod_4.1, nsamples = 1000)

# Model summary
summary(mod_4.1) 

# Plot model to get trace plots
plot(mod_4.1) 

# Hypothesis testing
plot(hypothesis(mod_4.1,"Gender1 = Gender0")) #I hypothized that there is no difference in resolution between women and men - which is not the case
plot(hypothesis(mod_4.1,"Gender1 > Gender0")) # After trying different hypotheses, this turned out to be the best

# Rank trace plots
mcmc_rank_overlay(mod_4.1, 
                  pars = c("b_Gender0", "b_Gender1")) + 
  theme_classic()

# Plot conditional effects
conditional_effects(mod_4.1)
```


Mod 5
Benefit ~ 1 + Susceptibility 
Hyp: Positive effect of susceptibility
```{r}
# Choose the variables needed.
df_5 <- data %>% select(crit_Susceptibility, Benefit)

# Set seed so that the analysis can be re-run and give the same results
set.seed(555)

# Define the model
mod_5 <- bf(Benefit ~ 1 + crit_Susceptibility)


# Figure out what priors we'll need:
get_prior(mod_5, family = gaussian, df_5) # We get Beta, Intercept and Sigma

#Checking range, mean and standard deviatin of benefit, to figure out which priors to choose and determine which family to choose
range(df_5$Benefit)
mean(df_5$Benefit)
sd(df_5$Benefit)

# Define priors
prior_mod_5 <- c(
  prior(normal(7.21, 31.31),     class = b), #mean and sd of benefit
  prior(normal(7.21, 31.31),     class = Intercept), #mean and sd of benefit
  prior(normal(31.31, 15.66),    class = sigma) #mean: sd of benefit. sigma: sd of benefit/2
)

# Fitting the model samplig only from priors, in order to check the quality of the priors
mod_5_0 <- brm(
  formula = mod_5, 
  prior = prior_mod_5,
  data = df_5,
  chains = 2,
  cores = 2,
  sample_prior = "only"
)


# Prior predictive check
pp_check(mod_5_0, nsamples = 1000) # 


# The actual model:
mod_5_1 <- brm(
  formula = mod_5, 
  prior = prior_mod_5,
  data = df_5,
  chains = 2,
  cores = 2,
  sample_prior = T,
  iter = 4000,
  control = list(adapt_delta=0.99, max_treedepth=20)
)

# Posterior predictive check
pp_check(mod_5_1, nsamples = 1000)

# Model summary
summary(mod_5_1)

# Plot the model to get trace plots
plot(mod_5_1) 


# Hypothesis testing
plot(hypothesis(mod_5_1,"crit_Susceptibility > 0")) #I hypothize that there is an effect of critical susceptibility on benefit - which is definitely the case

plot(hypothesis(mod_5_1,"Intercept > 0"))


# Rank trace plots
mcmc_rank_overlay(mod_5_1, 
                  pars = c("b_crit_Susceptibility")) + 
  theme_classic()

# Plot conditional effects
conditional_effects(mod_5_1)
```


Mod 6 
Susceptibility ~ 1 + Resolution + Calibration + Gender
Hyp: small positive effect of res, neg effect of cal, pos effect of women
```{r}
# We use df_meta, as it contains all the necessary variables

# Set seed so that the analysis can be re-run and give the same results
set.seed(555)

# Define the model
mod_6 <- bf(crit_Susceptibility ~ 1 + Resolution + Calibration + Gender)#


# Figure out what priors we'll need
get_prior(mod_6, family = gaussian, df_meta) # We get Beta, Intercept and Sigma

# To get an idea of how to put the priors
range(df_meta$Susceptibility)
mean(df_meta$Susceptibility)
sd(df_meta$Susceptibility)

# Define the model
prior_mod_6 <- c(
  prior(normal(0, .1),     class = b), #mean and sd of susceptibility - I choose one skeptical prior for all of the predicters
  prior(normal(0, .1),     class = Intercept), #mean and sd of susceptibility
  prior(normal(.1, .05),   class = sigma) #mean: sd of susceptibility. sigma: sd of susceptibility/2.
)


# Fitting the model samplig only from priors, in order to check the quality of the priors
mod_6_0 <- brm(
  formula = mod_6, 
  prior = prior_mod_6,
  data = df_meta,
  chains = 2,
  cores = 2,
  sample_prior = "only"
)


# Prior predictive check
pp_check(mod_6_0, nsamples = 1000) # 


# The actual model:
mod_6_1 <- brm(
  formula = mod_6, 
  prior = prior_mod_6,
  data = df_meta,
  chains = 2,
  cores = 2,
  sample_prior = T,
  iter = 4000,
  control = list(adapt_delta=0.99, max_treedepth=20)
)

# Posterior predictive check
pp_check(mod_6_1, nsamples = 1000)

# Model summary
summary(mod_6_1) 

# Plot the model to get trace plots
plot(mod_6_1) 


# Hypothesis testing
plot(hypothesis(mod_6_1,"Intercept > 0"))

plot(hypothesis(mod_6_1,"Calibration < 0")) #I hypothize that there is a negative effect of calibration crit susceptibility - which is the case

plot(hypothesis(mod_6_1,"Resolution > 0")) #I hypothize that there is a positive effect of resolution on crit susceptibility - which is not the case 
plot(hypothesis(mod_6_1,"Resolution < 0")) # After trying different hypotheses, this turned out to be the best

plot(hypothesis(mod_6_1,"Gender1 > 0")) #I hypothize that there is a pos effect of women on crit susceptibility - which is not the case
plot(hypothesis(mod_6_1,"Gender1 < 0")) # After trying different hypotheses, this turned out to be the best


# Rank trace plots
mcmc_rank_overlay(mod_6_1, 
                  pars = c("b_Calibration", "b_Resolution", "b_Gender1")) + 
  theme_classic()

# Plot conditional effects
conditional_effects(mod_6_1, pars = c("b_Calibration", "b_Resolution", "b_Gender1"))
```

